# Diagnose a Spark on MaxCompute job {#concept_263782 .concept}

This topic describes how to diagnose a Spark on MaxCompute job based on the job log. You can use LogView or the Spark Web UI to check whether a Spark on MaxCompute job is submitted or run.

## Background information {#section_58m_vcp_4ho .section}

When you submit a Spark on MaxCompute job by running the `spark-submit` script, MaxCompute creates an instance and adds instance information to the LogView log.

Specifically, you can submit a Spark on MaxCompute job through running the following`spark-submit`script:

``` {#codeblock_ptt_kpx_imm}
cd $SPARK_HOME
bin/spark-submit --master yarn-cluster --class  SparkPi /tmp/spark-2.x-demo/target/AliSpark-2.x-quickstart-1.0-SNAPSHOT-shaded.jar
```

After the job is submitted, MaxCompute creates an instance and adds the instance information to the LogView log as follows:

``` {#codeblock_tya_hmb_2ji}
19/01/05 20:36:47 INFO YarnClientImplUtil: logview url: http://logview.odps.aliyun.com/logview/?h=http://service.cn.maxcompute.aliyun.com/api&p=qn_beijing&i=xxx&token=xxx
If the job is submitted, the logged information includes but is not limited to the following:
19/01/05 20:37:34 INFO Client:
   client token: N/A
   diagnostics: N/A
   ApplicationMaster host: 11.220.xxx.xxx
   ApplicationMaster RPC port: 30002
   queue: queue
   start time: 1546691807945
   final status: SUCCEEDED
   tracking URL: http://jobview.odps.aliyun.com/proxyview/jobview/?h=http://service.cn.maxcompute.aliyun-inc.com/api&p=project_name&i=xxx&t=spark&id=application_xxx&metaname=xxx&token=xxx
```

**Note:** When you run a Spark on MaxCompute task in DataWorks, a similar log is created.

## Diagnose a Spark on MaxCompute job by using LogView {#section_1jp_5nc_ofs .section}

1.  Open a browser. Then use LogView to view the basic information about your Spark on MaxCompute job, a cupid-type task.

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/217830/156404375347096_en-US.jpg)

2.  On the**Fuxi Jobs**tab in the upper pane, find the task named**master-0**in the**TaskName**column. Then, in the lower pane, click the**ALL**tab.

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/217830/156404375447097_en-US.jpg)

3.  On the **TempRoot** tab, find the log you want to view, and click the icon in the **StdOut** column. Then you can view the log details generated by SparkPi.

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/217830/156404375447098_en-US.jpg)


## Diagnose a Spark on MaxCompute job by using the Spark Web UI {#section_e1m_v1q_tv6 .section}

If the log for a Spark on MaxCompute job contains a tracking URL, the job is submitted to the MaxCompute cluster. Both the Spark Web UI and the History Server use this tracking URL.

1.  Open a browser and enter the tracking URL in the address bar to track your Spark on MaxCompute job.

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/217830/156404375447099_en-US.jpg)

2.  Find the driver you want to view, and then click **stdout** in the **Logs** column.

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/217830/156404375447100_en-US.jpg)


