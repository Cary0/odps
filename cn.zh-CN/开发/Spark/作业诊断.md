# 作业诊断 {#concept_263782 .concept}

提交作业后，您需要根据作业日志来检查作业是否已正常提交并执行。MaxCompute为Spark作业提供了Logview工具以及Spark Web-UI，帮助开发者进行作业诊断。

通过Spark-submit方式（Dataworks执行Spark任务时也会产生相应日志）提交作业。

``` {#codeblock_i6a_0eu_ey4}
cd $SPARK_HOME
bin/spark-submit --master yarn-cluster --class  SparkPi /tmp/spark-2.x-demo/target/AliSpark-2.x-quickstart-1.0-SNAPSHOT-shaded.jar
```

作业提交成功后，MaxCompute会创建一个Instance，在日志中会打印Instance的Logview。

``` {#codeblock_rtn_d90_emz}
19/01/05 20:36:47 INFO YarnClientImplUtil: logview url: http://logview.odps.aliyun.com/logview/?h=http://service.cn.maxcompute.aliyun.com/api&p=qn_beijing&i=xxx&token=xxx
成功标准: <看到以下输出，可能会有其他日志一并输出>
19/01/05 20:37:34 INFO Client:
   client token: N/A
   diagnostics: N/A
   ApplicationMaster host: 11.220.xxx.xxx
   ApplicationMaster RPC port: 30002
   queue: queue
   start time: 1546691807945
   final status: SUCCEEDED
   tracking URL: http://jobview.odps.aliyun.com/proxyview/jobview/?h=http://service.cn.maxcompute.aliyun-inc.com/api&p=project_name&i=xxx&t=spark&id=application_xxx&metaname=xxx&token=xxx
```

## 使用Logview工具诊断作业 {#section_1jp_5nc_ofs .section}

1.  通过日志输出的Logview，在浏览器中可以查看CUPID类型的任务执行的基本信息。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/217830/156032895647096_zh-CN.jpg)

2.  单击**TaskName**下名为master-0的任务条，在下方**FuxiInstance**栏中，通过**All**过滤。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/217830/156032895747097_zh-CN.jpg)

3.  单击TempRoot的**StdOut**，可以查看SparkPi的输出结果。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/217830/156032895747098_zh-CN.jpg)


## 使用Web-UI诊断作业 {#section_e1m_v1q_tv6 .section}

日志中打印出上述的TrackingUrl，表示您的作业已经提交到MaxCompute集群。这个TrackingUrl非常关键，它既是SparkWebUI，也是HistoryServer的URL。

1.  在浏览器中打开这个URL，可以追踪Spark作业的运行情况。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/217830/156032895747099_zh-CN.jpg)

2.  单击driver对应的**stdout**即可以查看Spark作业的输出内容。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/217830/156032895847100_zh-CN.jpg)


