# 搭建开发环境 {#concept_263311 .concept}

本文为您介绍如何搭建MaxCompute Spark开发环境。

## 下载MaxCompute Spark客户端 {#section_732_97j_dh3 .section}

MaxCompute Spark发布包集成了MaxCompute认证功能。作为客户端工具，它用于通过Spark-submit方式提交作业到MaxCompute项目中运行。目前提供了面向Spark1.x和Spark2.x的2个发布包：

-   [spark-1.6.3](http://repo.aliyun.com/download/spark-1.6.3-public.tar.gz)
-   [spark-2.3.0](http://repo.aliyun.com/download/spark-2.3.0-public.tar.gz)

如果您需要开发Spark1.x应用，请使用spark-1.6.3版本客户端；如果您需要开发Spark2.x应用，请使用spark-2.3.0版本客户端。

## 设置环境变量 {#section_h50_2mv_513 .section}

-   JAVA\_HOME设置

    ``` {#codeblock_f02_ahy_eis .language-java}
    # 尽量使用JDK 1.7+
    # 1.8+ 最佳
    export JAVA_HOME=/path/to/jdk
    export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
    export PATH=$JAVA_HOME/bin:$PATH
    ```

-   SPARK\_HOME设置

    ``` {#codeblock_sr8_wy9_7ni .language-java}
    export SPARK_HOME=/path/to/spark_extracted_package
    export PATH=$SPARK_HOME/bin:$PATH
    ```

-   PySpark的用户请安装Python2.7版本，并设置PATH。

    ``` {#codeblock_67c_ym0_s5i}
    export PATH=/path/to/python/bin/:$PATH
    ```


## 配置Spark-defaults.conf {#section_9i8_ixj_ntx .section}

$SPARK\_HOME/conf路径下存在spark-defaults.conf.template文件，它可以作为spark-defaults.conf的模版。您需要在该文件中设置MaxCompute相关的账号及服务域名后，才可以提交Spark任务到MaxCompute。默认配置内容如下，将空白处填入实际的账号信息，其余的配置信息可保持不变。

``` {#codeblock_3ur_xq5_zy1}
# 配置MaxCompute项目及访问账号信息。
spark.hadoop.odps.project.name =
spark.hadoop.odps.access.id =
spark.hadoop.odps.access.key =

# 配置Spark客户端连接访问MaxCompute项目的endpoint，请根据网络环境以及region填写对应MaxCompute Endpoint。
spark.hadoop.odps.end.point = http://service.cn.maxcompute.aliyun.com/api
# Spark运行环境endpoint，请配置为所在region的MaxCompute VPC内网Endpoint
spark.hadoop.odps.runtime.end.point = http://service.cn.maxcompute.aliyun-inc.com/api

# 以下配置保持不变。
spark.sql.catalogImplementation=odps
spark.hadoop.odps.task.major.version = cupid_v2
spark.hadoop.odps.cupid.container.image.enable = true
spark.hadoop.odps.cupid.container.vm.engine.type = hyper
```

## 配置依赖 {#section_oey_mqy_cq1 .section}

-   配置访问MaxCompute表所需的依赖

    Spark作业访问MaxCompute表，需要依赖odps-spark-datasource模块。Maven坐标为：

    ``` {#codeblock_xc0_vd1_ga2}
    <!-- Spark-2.x请依赖此模块 -->
    <dependency>
        <groupId>com.aliyun.odps</groupId>
        <artifactId>odps-spark-datasource_2.11</artifactId>
        <version>3.3.3-public</version>
    </dependency>
    
    <!-- Spark-1.x请依赖此模块 -->
    <dependency>
      <groupId>com.aliyun.odps</groupId>
      <artifactId>odps-spark-datasource_2.10</artifactId>
      <version>3.3.3-public</version>
    </dependency>
    ```

-   配置访问OSS所需的依赖

    若作业需要访问OSS，直接添加以下依赖即可。

    ``` {#codeblock_2eh_eqv_llt}
    <dependency>
        <groupId>com.aliyun.odps</groupId>
        <artifactId>hadoop-fs-oss</artifactId>
        <version>3.3.3-public</version>
    </dependency>
    ```


