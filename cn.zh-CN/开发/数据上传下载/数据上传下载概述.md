# 数据上传下载概述 {#concept_am2_p3f_vdb .concept}

本文为您概述如何将数据上传至MaxCompute或从MaxCompute下载数据，包括服务连接、SDK、工具和数据导入导出、上云等常见操作。

总的来说，您可以通过DataHub实时数据通道和Tunnel批量数据通道两种途径进出MaxCompute系统。DataHub和Tunnel各自都提供了SDK，而基于这些SDK又衍生了许多用于数据上传下载的工具，方便您在各种场景下的数据进行上传/下载的需求。

数据上传下载的工具主要包括：DataWorks、DTS、OGG插件、Sqoop、Flume插件、LogStash插件、Fluentd插件、Kettle插件以及MaxCompute客户端等。

上述工具使用的底层数据通道，分类如下：

-   DataHub通道系列
    -   OGG插件
    -   Flume插件
    -   LogStash插件
    -   Fluentd插件
-   Tunnel通道系列
    -   DataWorks
    -   DTS
    -   Sqoop
    -   Kettle插件
    -   MaxCompute客户端

基于上述丰富的数据上传/下载的工具，可以满足大部分常见的数据上云场景，后续的章节会对工具本身以及[Hadoop数据迁移](../../../../intl.zh-CN/最佳实践/数据迁移/Hadoop数据迁移MaxCompute最佳实践.md#)、数据库数据同步、日志采集等数据上云的场景进行介绍，为您进行技术方案选型时提供参考。

**说明：** 对于离线数据的同步，推荐您优先使用[数据集成](../../../../intl.zh-CN/使用指南/数据集成/数据集成简介/数据集成概述.md#)。

## 使用限制 {#section_epm_d5m_sfb .section}

Tunnel Upload命令上传限制

-   上传不设速度限制，上传速度的瓶颈在网络带宽以及服务器性能。
-   重传retry有次数的限制，当重传次数超过限制，就会继续上传下一个block。上传完成后，可以通过`select count(*)`语句，检查是否有数据丢失。
-   Tunnel命令不支持上传下载Array、Map和Struct类型的数据。
-   Tunnel一个项目支持并发的连接数默认最高为2000个。
-   每个Tunnel的Session在服务端的生命周期为24小时，创建后24小时内均可使用，也可以跨进程/线程共享使用，但是必须保证同一个BlockId没有重复使用。
-   当遇到并发写入时，MaxCompute会根据ACID进行并发写的保障。关于ACID的具体语义，请参见[ZH-CN\_TP\_236123\_V1.md\#](intl.zh-CN/产品简介/基本概念/ACID语义.md#)。

DataHub上传数据限制

-   每个字段的大小不能超过这个字段本身的限制，详情请参见[数据类型](intl.zh-CN/开发/数据类型.md#)。

    **说明：** STRING的长度不能超过8M。

-   上传的过程中，会将多条数据打包成一个Package来进行上传。

TableTunnel SDK接口限制

-   BlockId的取值范围是\[0, 20000\)，单个block上传的数据限制为100G。
-   Session的超时时间为24小时。如果，大批量数据传送导致超过24小时，需要自行拆分成多个Session。
-   RecordWriter对应的HTTP Request超时为120s。如果120s内HTTP连接上没有数据流过，service端会主动关闭连接。

