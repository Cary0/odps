# 前提条件 {#concept_f1r_5tc_kgb .concept}

本章节将指导用户使用MaxCompute Spark快速构建Spark on MaxCompute的应用，并在MaxCompute项目中提交执行Spark作业。

## 下载MaxCompute Spark发布包 {#section_zsg_2rb_kgb .section}

-   [spark-1.6.3](http://repo.aliyun.com/download/spark-1.6.3-public.tar.gz)
-   [spark-2.3.0](http://repo.aliyun.com/download/spark-2.3.0-public.tar.gz)

通过MaxCompute Spark发布包可以在本地通过spark-submit方式提交作业。如果您需要使用Spark-1.x版本，请选择spark-1.6.3发布包。需要Spark-2.x请下载spark-2.3.0发布包。

MaxCompute集群同时支持spark1.x、spark2.x，服务侧将根据用户提交任务的MaxCompute Spark版本自动启动对应版本服务。

发布包的目录结构和社区的Spark完全一致，其中conf目录下有spark-defaults.conf作为默认配置参数的文件。bin目录下目前只支持使用spark-submit来提交作业，spark-shell、spark-sql目前都还不支持。

```language-java
1.
2.|-- RELEASE
3.|-- bin
4.|-- conf
5.|-- lib
6.|-- python
7.`-- sbin

```

## 环境准备 {#section_sx3_hmf_4gb .section}

-   JAVA\_HOME设置

    ```language-java
    # 尽量使用JDK 1.7+
    # 1.8+ 最佳
    export JAVA_HOME=/path/to/jdk
    export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
    export PATH=$JAVA_HOME/bin:$PATH
    ```

-   SPARK\_HOME设置

    ```language-java
    export SPARK_HOME=/path/to/spark_extracted_package
    export PATH=$SPARK_HOME/bin:$PATH
    ```


本地开发环境有Maven3.3.3以上。

