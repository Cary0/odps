# 运行模式 {#concept_e1h_35c_kgb .concept}

目前MaxCompute Spark支持以下几种运行方式：local模式，cluster模式，和在DataWorks中执行模式。

## 运行模式说明 {#section_ybd_fbc_kgb .section}

-   Local模式

    Local模式主要是让用户能够方便的调试应用代码，使用方式跟社区相同，添加了用tunnel读写ODPS表的功能。您可以在IDE和命令行中使用该模式，需要添加配置`spark.master=local[N]`，其中N表示执行该模式所需的cpu资源。此外，local模式下读写表是通过读写tunnel完成的，需要在Spark-defaults.conf中增加tunnel配置项（请根据MaxCompute项目所在的region及网络环境填写对应的[Tunnel Endpoint地址](https://help.aliyun.com/document_detail/34951.html)）。命令行执行该模式的方式如下：

    ``` {#codeblock_hz3_0he_mt7 .language-php}
    1.bin/spark-submit --master local[4] \
    --class com.aliyun.odps.spark.examples.SparkPi \
    ${path to aliyun-cupid-sdk}/spark/spark-2.x/spark-examples/target/spark-examples_2.11-version-shaded.jar
    ```

-   Cluster模式

    在cluster模式中，用户需要指定自定义程序入口main。Main结束（Success or Fail），对应的spark作业就会结束。使用场景适合于离线作业，可与阿里云DataWorks产品结合进行作业调度。命令行提交方式如下：

    ```language-java
    1.bin/spark-submit --master yarn-cluster \
    –class SparkPi \
    ${ProjectRoot}/spark/spark-2.x/spark-examples/target/spark-examples_2.11-version-shaded.jar
    ```

-   DataWorks执行模式

    **说明：** DataWorks的Spark节点目前在灰度发布中，如果您有在DataWorks中调用Spark节点的需求，请通过工单或加入钉钉群：21969532（MaxCompute Spark支持群）进行沟通、申请。

    用户可以在DataWorks中运行MaxCompute Spark离线作业（cluster模式），以方便与其他类型执行节点集成和调度。具体步骤如下：

    1.  用户需要在DataWorks的业务流程中上传并提交（单击**提交**按钮）资源：

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/155771146336706_zh-CN.png)

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/155771146336708_zh-CN.png)

    2.  在创建的业务流程中，从**数据开发**组件中选择**ODPS Spark**节点。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/155771146336713_zh-CN.png)

    3.  双击拖拽到工作流的Spark节点，对Spark作业进行任务定义。选择Spark的版本、任务使用的开发语言，并指定任务所使用的资源文件。这里的资源文件就是第一步在业务流程中预先上传并发布的资源文件。同时，您还可以指定提交作业时的配置项，如executor的数量、内存大小等配置项。同时还需设置MaxCompute Spark服务的endpoint配置项：`spark.hadoop.odps.cupid.webproxy.endpoint`（取值填写项目所在region的endpoint，例如http://service.cn.maxcompute.aliyun-inc.com/api）。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/155771146336717_zh-CN.png)

    4.  手动执行Spark节点，可以查看该任务的执行日志，从打印出来的日志中可以获取该任务的logview和jobview的url，编译进一步查看与诊断。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/155771146436724_zh-CN.png)

        Spark作业定义完成后，即可在业务流程中对不同类型服务进行编排、统一调度执行。


