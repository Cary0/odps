# 常见问题 {#concept_y5c_w5c_kgb .concept}

本小节总结Spark使用过程中的常见问题。

-   Q：spark-defaults.conf提供的id、key有问题，提示信息如下。

    ```language-java
    Stack:
    com.aliyun.odps.OdpsException: ODPS-0410042:
    Invalid signature value - User signature dose not match
    ```

    A：请检查spark-defaults.conf提供的id、key和在阿里云官网管理控制台[用户信息管理](https://account.aliyun.com/login/login.htm?oauth_callback=https://usercenter.console.aliyun.com/)中的**AccessKey ID**、**Access Key Secret**是否一致。

-   Q：提示权限不足 。

    ```language-java
    Stack:
    com.aliyun.odps.OdpsException: ODPS-0420095: 
    Access Denied - Authorization Failed [4019], You have NO privilege 'odps:CreateResource' on {acs:odps:*:projects/*}
    ```

    A：请project owner授权grant resource的read以及create权限。

-   Q：项目未支持Spark任务运行，提示信息如下。

    ```language-java
    Exception in thread "main" org.apache.hadoop.yarn.exceptions.YarnException: com.aliyun.odps.OdpsException: ODPS-0420095: Access Denied - The task is not in release range: CUPID
    ```

    A：首先需要确认项目所在的region中，是否已经提供了MaxCompute Spark服务。同时，检查Spark-defaults.conf配置信息是否与产品文档中的要求一致。如果region已经支持，请通过工单或加入钉钉群：21969532（MaxCompute Spark支持群）进行咨询。

-   Q：运行时报错信息为：`No space left on device`。

    A：Spark使用网盘进行本地存储。Shuffle数据和BlockManager溢出的数据均存储在网盘上。网盘的大小通过参数spark.hadoop.odps.cupid.disk.driver.device\_size控制，默认20g，最大100g。如果调整到100g仍然报出此错误，则需要分析具体原因。常见的原因为数据倾斜，在shuffle或者cache过程中数据集中分布在某些block，此时可以缩小单个executor的并发（spark.executor.cores），增加executor的数量（spark.executor.instances）。

-   Q：如何通过Spark访问VPC环境内服务？

    A：Spark默认支持对MaxCompute表的访问，支持对OSS、OTS等外部数据源的访问处理。对VPC内的服务访问，目前默认暂不支持访问，如有需求可以通过工单和我们联系。

-   Q：如何把开源Spark代码迁移到Spark on MaxCompute？

    A：分以下三种情形：

    -   作业无需访问MaxCompute表和OSS。

        用户jar包可直接运行，参照第二节准备开发环境和修改配置。注意，对于Spark或Hadoop的依赖必须设成provided。

    -   作业需要访问MaxCompute表。

        配置相关依赖后重新打包即可。配置依赖的步骤请参见[配置依赖](cn.zh-CN/用户指南/Spark/搭建开发环境.md#section_oey_mqy_cq1)。

    -   作业需要访问OSS。

        配置相关依赖后重新打包即可。配置依赖的步骤请参见[配置依赖](cn.zh-CN/用户指南/Spark/搭建开发环境.md#section_oey_mqy_cq1)。


