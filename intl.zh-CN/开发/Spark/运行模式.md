# 运行模式 {#concept_e1h_35c_kgb .concept}

目前MaxCompute Spark支持以下三种运行方式：Local模式、Cluster模式和在DataWorks中执行模式。

## Local模式 {#section_1tq_j9h_c6m .section}

Local模式主要是为方便用户调试应用代码。在Local模式下，MaxCompute Spark的使用方式与社区相同，还添加了用Tunnel读写MaxCompute表的功能。您可以在IDE和命令行中使用该模式需要添加配置`spark.master=local[N]`，其中N表示执行该模式所需的CPU资源。Local模式下读写表是通过读写Tunnel完成的，需要在Spark-defaults.conf中增加Tunnel配置项（请根据MaxCompute项目所在的Region及网络环境填写对应的Endpoint地址，详情请参见[配置Endpoint](../../../../intl.zh-CN/准备工作/配置Endpoint.md#)）。命令行执行该模式的方式如下。

``` {#codeblock_029_y8r_40c .language-php}
1.bin/spark-submit --master local[4] \
--class com.aliyun.odps.spark.examples.SparkPi \
${path to aliyun-cupid-sdk}/spark/spark-2.x/spark-examples/target/spark-examples_2.11-version-shaded.jar
```

## Cluster模式 {#section_anp_rmp_mpx .section}

在Cluster模式中，您需要指定自定义程序入口main。main结束（Success or Fail）时，对应的Spark作业就会结束。使用场景适合于离线作业，可与阿里云DataWorks产品结合进行作业调度。命令行提交方式如下。

``` {#codeblock_ri0_mv6_38o .language-java}
1.bin/spark-submit --master yarn-cluster \
–class SparkPi \
${ProjectRoot}/spark/spark-2.x/spark-examples/target/spark-examples_2.11-version-shaded.jar
```

## DataWorks执行模式 {#section_uqe_uwt_q0b .section}

**说明：** DataWorks的Spark节点目前已经支持的Region为：杭州、北京、上海、深圳、成都、中国（香港）、美西、法兰克福、孟买、新加坡。

用户可以在DataWorks中运行MaxCompute Spark离线作业（Cluster模式），以方便与其它类型执行节点集成和调度。具体步骤如下：

1.  用户需要在DataWorks的业务流程中上传并提交（单击**提交**按钮）资源。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/156593954636706_zh-CN.png)

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/156593954636708_zh-CN.png)

2.  在创建的业务流程中，从**数据开发**组件中选择**ODPS Spark**节点。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/156593954636713_zh-CN.png)

3.  双击拖拽到工作流的Spark节点，对Spark作业进行任务定义。选择Spark的版本、任务使用的开发语言，并指定任务所使用的资源文件。这里的资源文件就是第一步在业务流程中预先上传并发布的资源文件。同时，您还可以指定提交作业时的配置项，例如Executor的数量、内存大小等配置项。同时还需设置MaxCompute Spark服务的Endpoint配置项：`spark.hadoop.odps.cupid.webproxy.endpoint`（取值填写项目所在Region的Endpoint，例如http://service.cn.maxcompute.aliyun-inc.com/api）。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/156593954636717_zh-CN.png)

4.  手动执行Spark节点，可以查看该任务的执行日志，从打印出来的日志中可以获取该任务的Logview和Jobview的URL，编译进一步查看与诊断。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/92656/156593954736724_zh-CN.png)

    Spark作业定义完成后，即可在业务流程中对不同类型服务进行编排、统一调度执行。


