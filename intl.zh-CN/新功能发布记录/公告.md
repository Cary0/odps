# 公告 {#concept_p4b_ndc_5db .concept}

本文为您提供关于MaxCompute使用功能的各项更新记录。

## 2019年8月29日-外表自定义storagehandler实现Outputer接口升级 {#section_mbc_l25_704 .section}

北京时间2019年8月29日，MaxCompute进行版本升级。期间，您在使用外表自定义storagehandler实现Outputer接口时，如果通过列名而非数字下标获取列数据，可能会引起作业失败。

升级时间：北京时间，2019年8月29日14:00-23:00

升级Region：美国西部1（硅谷）、亚太东南1（新加坡）

## 2019年8月21日-外表自定义storagehandler实现Outputer接口升级 {#section_bjl_f2x_mqc .section}

北京时间2019年8月21日，MaxCompute进行版本升级。期间，您在使用外表自定义storagehandler实现Outputer接口时，如果通过列名而非数字下标获取列数据，可能会引起作业失败。

升级时间：北京时间，2019年8月21日14:00-23:00

升级Region：亚太东北1（东京）、欧洲中部1（法兰克福）、中国（香港）、亚太东南2（悉尼）

影响原因：`Outputer.output(Record record)中`，传入的record为Outputer的上一个Operator产生的记录，列名可能发生变化，系统无法保证固定列名。

例如，表达式`some_function(column_a)`产生的列名是一个临时列名。因此，使用`record.get(列名)`方式来获取列内容的用法都有可能受到影响，建议使用`record.get(index)方式`获取。Outputer里如需获取表的列名，请调用 `DataAttributes.getFullTableColumns()`。

如您遇到相关问题，请提交工单咨询。

## 2019年7月24日-MaxCompute Spark开放 {#section_9w8_2bf_xs0 .section}

开放Region：华东1、华北2、华南1、美西1、中国（香港）、德国、新加坡、印度。

## 2019年3月26日-SQL语言功能升级 {#section_jy1_s1h_fhb .section}

-   支持Grouping Set多维聚合分析（Cube，Roll up），适配需要对a列做聚合，也要对b列做聚合，或者同时要按照a、b两列做聚合的场景。具体的使用方法请参见[GROUPING SETS](../../../../intl.zh-CN/开发/SQL及函数/SELECT语句/GROUPING SETS.md#)。
-   支持INTERSECT/MINUS/EXCEPT，具体使用方法请参见[交集、并集和补集](../../../../intl.zh-CN/开发/SQL及函数/SELECT语句/交集、并集和补集.md#)。
-   通过外表读取OSS上的ORC和Parquet格式的文件时，支持对文件的列裁剪，有效减少IO量，节省计算资源和成本。
-   Java UDX类型系统增强：UDF支持Writable参数，具体使用方法请参见[Java UDF](../../../../intl.zh-CN/开发/SQL及函数/UDF/Java UDF.md#)。

 **SQL性能优化** 

-   DynamicDAG：动态优化的必要机制，它可以将一个优化（可能是资源配置或算法选择）延迟到运行时，以便提高优化的精准度，降低产生较差计划的可能性。
-   ShuffleRemove Optimization：针对Shuflle的优化。 从本次版本升级开始，MaxCompute将支持对inner join时右表key相同的场景进行Shuffle Remove优化。

## 2019年3月1日-MaxCompute 外部表开始收费 {#section_prm_zcd_5gb .section}

从2019年3月1日开始，MaxCompute SQL外部表（处理OSS/Table Store数据）功能开始计费。

计费标准为：

``` {#codeblock_rw4_5v4_y2n}
一次SQL计算费用=计算输入数据量*SQL价格
```

SQL价格为0.0044 USD/GB。当天的所有计量信息在第二天做一次性汇总收费，并直接体现在您的账户账单中。详情请参见[SQL外表计费](../../../../intl.zh-CN/产品定价/计算计费.md#section_85k_bnk_4a8)。如有疑问，您可提交工单咨询。

## 2019年1月15日16:00-20:00-中国（香港）Region底层优化 {#section_pts_yhk_lgb .section}

为向您提供更好的产品性能和稳定性，MaxCompute将于北京时间2019年1月15日16:00 - 20:00对中国（香港）Region底层元数据仓库组件进行优化。在优化期间，中国（香港）Region用户的应用可能出现1分钟左右任务无法提交、运行中任务失败的情况。极端情况下，应用不可用时间将延长至30分钟。请您尽量避免在迁移窗口提交作业。其他Region不受影响。如果您有任何问题，可随时通过企业钉钉群或工单反馈。

## 2018年12月24日-MaxCompute支持时区配置 {#section_wjq_mrc_ggb .section}

MaxCompute Project时区默认是中国的东八区，在用到DATETIME/TIMESTAMP/DATE类型相关的字段以及相关时间内置函数都是按东八区进行计算，2018年12月24日MaxCompute开始支持时区配置，主要两种方式：

-   Session级别： `set odps.sql.timezone=<timezoneid>;`该语句需要与计算语句一起提交，如：

    ``` {#codeblock_snj_sk8_jjw}
    set odps.sql.timezone=Asia/Tokyo;
    select getdate();
    --结果如下。
    output:
    +------------+
    | _c0        |
    +------------+
    | 2018-10-30 23:49:50 |
    +------------+
    ```

-   Project级别： `setProject odps.sql.timezone=<timezoneid>;`该语句需要Project Owner通过命令行方式执行。Project一旦设置之后对应相关的时间计算就会取设置后的时区，因此对于原有的任务数据会有影响，所以该操作需谨慎。若有需要建议对新增Project设置，已有数据的Project不建议设置。

使用限制及注意事项：

-   时区配置支持范围为：SQL内置日期函数、UDF、UDT、UDJ、select transform都支持获取Project Timezone属性来支持时区。
-   时区支持的格式为Asia/Shanghai这类格式（存在夏令时跳变），不支持GMT+9格式。
-   当SDK时区与Project时区不同时，DATETIME-\>STRING操作需设置GMT时区。
-   时区配置后，通过DataWorks执行相关SQL时，日期显示某些时间段会有时间差异：1900-1928年的日期时间差异5分52秒，1900年之前的日期时间差异9秒。
-   为了保证MaxCompute在多个时区DATETIME类型数据的正确性，MaxCompute服务、Java SDK以及配套的Console将在近期进行版本更新\(`-oversea`后缀的Java SDK/Console版本\)，更新后可能影响MaxCompute中已经存储的早于1928年的DATETIME类型数据的显示。
-   建议（非中国东八区region）同步更新Java SDK/Console版本，以便保证SQL计算结果及Tunnel传输数据在1900-01-01之后范围的准确性和一致性。而对于早于1900-01-01的DATETIME数据，SQL的计算显示结果和Tunnel传输数据仍旧可能存在343秒的差异。对于新版本SDK/Console之前已经上传的早于1928-01-01的DateTime数据，在新版本中会减少352秒。
-   如果继续沿用不带有`-oversea`后缀的SDK/Console，将存在SQL计算结果和Tunnel 传输数据存在差异的风险：早于1900-01-01的数据差异为9秒，1900-01-01~1928-01-01的数据差异为352秒。

    **说明：** Java SDK/Console版本更新配置时区不影响DataWorks的时区配置，因此时区会存在差异，需要您对DataWorks中定时任务调度的影响进行计算评估。在日本Region，DataWorks服务器是GMT+9，在新加坡Region，DataWorks服务器是GMT+8。

-   通过JDBC连接的第三方客户端需要在客户端设置时区，保证与服务端时区设置一致性。
-   MapReduce支持时区功能。
-   Spark支持时区功能。
    1.  对于提交到ODPS计算集群的任务形式，现在不用用户配置就可以自动获取Project时区了。
    2.  对于用户通过yarn-client模式启动（例如spark-shell，spark-sql，pyspark等）的设置，需要用户手动配置Driver的启动参数（spark-defaults.conf），增加`spark.driver.extraJavaOptions -Duser.timezone=America/Los_Angeles`，Timezone是要使用的时区。
-   PAI支持时区功能。
-   Graph支持时区功能。

