# 前提条件 {#concept_f1r_5tc_kgb .concept}

本章节将指导用户使用MaxCompute Spark快速构建Spark on MaxCompute的应用，并在MaxCompute项目中提交执行Spark作业。

## 下载MaxCompute Spark发布包 {#section_zsg_2rb_kgb .section}

-   [http://repo.aliyun.com/download/spark-1.6.3-public.tar.gz](http://repo.aliyun.com/download/spark-1.6.3-public.tar.gz)
-   [http://repo.aliyun.com/download/spark-2.3.0-public.tar.gz](http://repo.aliyun.com/download/spark-2.3.0-public.tar.gz)

通过使用MaxCompute Spark发布包，您可以在本地以Spark-submit方式提交作业。如果您需要使用Spark-1.x版本，请下载Spark-1.6.3发布包；如果您需要使用Spark-2.x版本，请下载Spark-2.3.0发布包。

MaxCompute集群同时支持Spark-1.x、Spark-2.x，服务器侧将根据用户提交任务的MaxCompute Spark版本自动启动对应版本的服务。

发布包的目录结构和社区的Spark完全一致，其中conf目录下有spark-defaults.conf作为默认配置参数的文件。目前，bin目录下只支持使用spark-submit提交作业，不支持使用spark-shell、spark-sql。

```
|-- bin
|-- conf
|-- cupid
|-- lib
|-- python
|-- sbin
|-- RELEASE			
```

## 环境准备 {#section_sx3_hmf_4gb .section}

-   JAVA\_HOME设置

    ```language-java
    # 尽量使用JDK 1.7+
    # 1.8+ 最佳
    export JAVA_HOME=/path/to/jdk
    export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
    export PATH=$JAVA_HOME/bin:$PATH
    ```

-   SPARK\_HOME设置

    ```language-java
    export SPARK_HOME=/path/to/spark_extracted_package
    export PATH=$SPARK_HOME/bin:$PATH
    ```


本地开发环境支持Maven3.3.3以上版本。

